{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e107bd7-032f-44b4-a95a-37c9b283e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from utils.utils import LoadDataset, set_seed\n",
    "from simclr.simclr_model import SimCLR\n",
    "from byol.byol_model import BYOL\n",
    "from moco.moco_model import MoCo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf513bf-2b19-49a3-afe5-16a1b056c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f0240-528c-49cf-95ac-463ddeb63ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_feature_vectors_from_simclr(model_path, data_loader):\n",
    "\n",
    "    \"\"\"Function to extract feature vectors from a SimCLR model\n",
    "    Parameters:\n",
    "    model_path (str): Path to the model\n",
    "    data_loader (torch.utils.data.DataLoader): Data loader for the dataset\n",
    "    Output:\n",
    "    feature_vectors (numpy array): Feature vectors extracted from the model\n",
    "    labels (numpy array): Labels of the feature vectors\n",
    "    \"\"\"\n",
    "\n",
    "    set_seed(42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    resnet = torchvision.models.resnet18()\n",
    "    backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    model = SimCLR(backbone)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    feature_vectors = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, label in data_loader:\n",
    "            images = images.to(device) \n",
    "            outputs = model.backbone(images).flatten(start_dim=1)\n",
    "            feature_vectors.append(outputs.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "    feature_vectors = np.concatenate(feature_vectors)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    return feature_vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e371eb1-0695-4dd9-82ba-7cf604592340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_feature_vectors_from_byol(model_path, data_loader):\n",
    "\n",
    "    \"\"\"Function to extract feature vectors from a BYOL model\n",
    "    Parameters:\n",
    "    model_path (str): Path to the model\n",
    "    data_loader (torch.utils.data.DataLoader): Data loader for the dataset\n",
    "    Output:\n",
    "    feature_vectors (numpy array): Feature vectors extracted from the model\n",
    "    labels (numpy array): Labels of the feature vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    resnet = torchvision.models.resnet18()\n",
    "    backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    model = BYOL(backbone)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    feature_vectors = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, label in data_loader:\n",
    "            images = images.to(device) \n",
    "            outputs = model.backbone(images).flatten(start_dim=1)\n",
    "            feature_vectors.append(outputs.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "    feature_vectors = np.concatenate(feature_vectors)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    return feature_vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449072eb-fdc0-4223-83d6-ab6d4f84df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_feature_vectors_from_moco(model_path, data_loader):\n",
    "\n",
    "    \"\"\"Function to extract feature vectors from a MoCo model\n",
    "    Parameters:\n",
    "    model_path (str): Path to the model\n",
    "    data_loader (torch.utils.data.DataLoader): Data loader for the dataset\n",
    "    Output:\n",
    "    feature_vectors (numpy array): Feature vectors extracted from the model\n",
    "    labels (numpy array): Labels of the feature vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    resnet = torchvision.models.resnet18()\n",
    "    backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    model = MoCo(backbone)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    feature_vectors = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, label in data_loader:\n",
    "            images = images.to(device) \n",
    "            outputs = model.backbone(images).flatten(start_dim=1)\n",
    "            feature_vectors.append(outputs.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "    feature_vectors = np.concatenate(feature_vectors)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    return feature_vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1826d-fe4a-4794-bd13-b1c71e2536d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simclr models, Replace the paths with the paths to your models paths on your machine\n",
    "simclr_models = {\"seed 0\": [\"/home/jovyan/models/trained_models/seed_zero/simclr/simclr_model_center_cropping.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_zero/simclr/simclr_model_random_cropping.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_zero/simclr/simclr_model_color_jitter.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_zero/simclr/simclr_model_random_flipping.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_zero/simclr/simclr_model_random_perspective.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_zero/simclr/simclr_model_random_rotation.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_zero/simclr/simclr_model_random_grayscale.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_zero/simclr/simclr_model_gaussian_blur.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_zero/simclr/simclr_model_random_invert.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_zero/simclr/simclr_model_random_erasing.pth\"], \n",
    "                 \n",
    "                \"seed 42\": [\"/home/jovyan/models/trained_models/seed_42/simclr/simclr_model_center_cropping.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/simclr/simclr_model_random_cropping.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/simclr/simclr_model_color_jitter.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/simclr/simclr_model_random_flipping.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/simclr/simclr_model_random_perspective.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/simclr/simclr_model_random_rotation.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/simclr/simclr_model_random_grayscale.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/simclr/simclr_model_gaussian_blur.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/simclr/simclr_model_random_invert.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/simclr/simclr_model_random_erasing.pth\"],\n",
    "                 \n",
    "                \"seed 123\": [\"/home/jovyan/models/trained_models/seed_123/simclr/simclr_model_center_cropping.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/simclr/simclr_model_random_cropping.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/simclr/simclr_model_color_jitter.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/simclr/simclr_model_random_flipping.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/simclr/simclr_model_random_perspective.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/simclr/simclr_model_random_rotation.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/simclr/simclr_model_random_grayscale.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/simclr/simclr_model_gaussian_blur.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/simclr/simclr_model_random_invert.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/simclr/simclr_model_random_erasing.pth\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfadc6f4-e5c8-4561-9b65-a9e6a47bad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byol models, Replace the paths with the paths to your models paths on your machine\n",
    "byol_models = {\"seed 0\": [\"/home/jovyan/models/trained_models/seed_zero/byol/byol_model_center_cropping.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/byol/byol_model_random_cropping.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/byol/byol_model_color_jitter.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/byol/byol_model_random_flipping.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/byol/byol_model_random_perspective.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/byol/byol_model_random_rotation.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/byol/byol_model_random_grayscale.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/byol/byol_model_gaussian_blur.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/byol/byol_model_random_invert.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/byol/byol_model_random_erasing.pth\"], \n",
    "                 \n",
    "                \"seed 42\": [\"/home/jovyan/models/trained_models/seed_42/byol/byol_model_center_cropping.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/byol/byol_model_random_cropping.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/byol/byol_model_color_jitter.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/byol/byol_model_random_flipping.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/byol/byol_model_random_perspective.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/byol/byol_model_random_rotation.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/byol/byol_model_random_grayscale.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/byol/byol_model_gaussian_blur.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/byol/byol_model_random_invert.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/byol/byol_model_random_erasing.pth\"],\n",
    "                 \n",
    "                \"seed 123\": [\"/home/jovyan/models/trained_models/seed_123/byol/byol_model_center_cropping.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/byol/byol_model_random_cropping.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/byol/byol_model_color_jitter.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/byol/byol_model_random_flipping.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/byol/byol_model_random_perspective.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/byol/byol_model_random_rotation.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/byol/byol_model_random_grayscale.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/byol/byol_model_gaussian_blur.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/byol/byol_model_random_invert.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/byol/byol_model_random_erasing.pth\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92988ed3-1abd-4620-9dd6-d3cd0546f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoCo models, Replace the paths with the paths to your models paths on your machine\n",
    "moco_models = {\"seed 0\": [\"/home/jovyan/models/trained_models/seed_zero/moco/moco_model_center_cropping.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/moco/moco_model_random_cropping.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/moco/moco_model_color_jitter.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/moco/moco_model_random_flipping.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/moco/moco_model_random_perspective.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/moco/moco_model_random_rotation.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/moco/moco_model_random_grayscale.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/moco/moco_model_gaussian_blur.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/moco/moco_model_random_invert.pth\", \n",
    "                          \"/home/jovyan/models/trained_models/seed_zero/moco/moco_model_random_erasing.pth\"], \n",
    "                 \n",
    "                \"seed 42\": [\"/home/jovyan/models/trained_models/seed_42/moco/moco_model_center_cropping.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/moco/moco_model_random_cropping.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/moco/moco_model_color_jitter.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/moco/moco_model_random_flipping.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/moco/moco_model_random_perspective.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/moco/moco_model_random_rotation.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/moco/moco_model_random_grayscale.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/moco/moco_model_gaussian_blur.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/moco/moco_model_random_invert.pth\", \n",
    "                            \"/home/jovyan/models/trained_models/seed_42/moco/moco_model_random_erasing.pth\"],\n",
    "                 \n",
    "                \"seed 123\": [\"/home/jovyan/models/trained_models/seed_123/moco/moco_model_center_cropping.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/moco/moco_model_random_cropping.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/moco/moco_model_color_jitter.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/moco/moco_model_random_flipping.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/moco/moco_model_random_perspective.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/moco/moco_model_random_rotation.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/moco/moco_model_random_grayscale.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/moco/moco_model_gaussian_blur.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/moco/moco_model_random_invert.pth\", \n",
    "                             \"/home/jovyan/models/trained_models/seed_123/moco/moco_model_random_erasing.pth\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a339ebe-8799-4dd4-b567-b8e3dfc1ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances_for_single_image(query_vector, feature_vectors):\n",
    "    \n",
    "    \"\"\"Function to compute the cosine similarity between a query vector and a set of feature vectors\n",
    "    Parameters:\n",
    "    query_vector (numpy array): Query vector\n",
    "    feature_vectors (numpy array): Feature vectors\n",
    "    Output:\n",
    "    distances (numpy array): Cosine similarity between the query vector and the feature vectors\n",
    "    \"\"\"\n",
    "\n",
    "    distances = cosine_similarity(query_vector.reshape(1, -1), feature_vectors)\n",
    "    return distances.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6cd9e5-98c4-46b3-8034-5a7aecda7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_name(model_path):\n",
    "\n",
    "    \"\"\"Function to format the model name\n",
    "    Parameters:\n",
    "    model_path (str): Path to the model\n",
    "    Output:\n",
    "    formatted_name (str): Formatted model name\n",
    "    \"\"\"\n",
    "    \n",
    "    model_name = model_path.split('/')[-1]\n",
    "    \n",
    "    for prefix in [\"simclr_model_\", \"byol_model_\", \"moco_model_\"]:\n",
    "        model_name = model_name.replace(prefix, \"\")\n",
    "    formatted_name = model_name.replace(\".pth\", \"\").replace(\"_\", \" \").capitalize()\n",
    "    \n",
    "    return formatted_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf19f77-6cf3-4f3f-be50-3030e4e3c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets, Replace the paths with the paths to your datasets paths on your machine\n",
    "binary_class_data =  LoadDataset(\"/home/jovyan/data/cat_dog/\", 50).load_data()\n",
    "vehicles_dataset = LoadDataset(\"/home/jovyan/data/vehicles/\", 50).load_data()\n",
    "clothing_dataset = LoadDataset(\"/home/jovyan/data/clothing/\", 50).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4dd6f-8a83-47bf-b8fc-e5e027f1c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"Cat vs Dogs\": binary_class_data,\n",
    "    \"Vehicles\": vehicles_dataset,\n",
    "    \"Clothing\": clothing_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64196130-9ac3-4004-a116-180e8dee412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison_nearest_neighbors(query_image, nearest_neighbors_images_by_seed, distances_by_seed, title):\n",
    "\n",
    "    \"\"\"Function to plot the nearest neighbors of a query image\n",
    "    Parameters:\n",
    "    query_image (torch.Tensor): Query image\n",
    "    nearest_neighbors_images_by_seed (dict): Nearest neighbors images by seed\n",
    "    distances_by_seed (dict): Distances by seed\n",
    "    title (str): Title of the plot\n",
    "    Output:\n",
    "    Plot of the nearest neighbors of the query image\n",
    "    \"\"\"\n",
    "    \n",
    "    num_seeds = len(nearest_neighbors_images_by_seed)\n",
    "    num_neighbors = len(next(iter(nearest_neighbors_images_by_seed.values())))\n",
    "\n",
    "    fig, axes = plt.subplots(num_seeds, num_neighbors + 1, figsize=(3 * (num_neighbors + 1), 3 * num_seeds))\n",
    "    fig.suptitle(title, fontsize=20, y=0.96, fontweight=\"bold\")\n",
    "    \n",
    "    for i, (seed, neighbors_images) in enumerate(nearest_neighbors_images_by_seed.items()):\n",
    "        axes[i, 0].imshow(query_image.permute(1, 2, 0).numpy())\n",
    "        axes[i, 0].set_title(f\"Query image\", fontsize=12, pad=10)\n",
    "        axes[i, 0].set_ylabel(f\"{seed}\", fontsize=16, fontweight=\"bold\")\n",
    "        axes[i, 0].set_xticks([]) \n",
    "        axes[i, 0].set_yticks([]) \n",
    "\n",
    "        for j, (neighbor_img, distance) in enumerate(zip(neighbors_images, distances_by_seed[seed])):\n",
    "            ax = axes[i, j + 1]\n",
    "            ax.imshow(neighbor_img.permute(1, 2, 0).numpy())\n",
    "            ax.axis(\"off\")\n",
    "            ax.text(0.5, 1.05, f\"Similarity: {distance:.8f}\", transform=ax.transAxes, \n",
    "                    fontsize=12, ha='center', va='bottom', color='black')\n",
    "    \n",
    "    path = f\"/home/jovyan/scripts/plots/saved_plots/nearest_neighbours/{title}.pdf\"\n",
    "    plt.savefig(path, bbox_inches='tight', dpi=50)\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0cc30c-b58d-4f85-b576-8ca08eee6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_plot_nearest_neighbors(model_name, models, datasets):\n",
    "\n",
    "    \"\"\"Function to process and plot the nearest neighbors of a query image\n",
    "    Parameters:\n",
    "    model_name (str): Name of the model\n",
    "    models (dict): Models\n",
    "    datasets (dict): Datasets\n",
    "    Output:\n",
    "    Plot of the nearest neighbors of the query image\n",
    "    \"\"\"\n",
    "\n",
    "    for dataset_name, dataloader in datasets.items():\n",
    "        for model_paths in zip(*models.values()):\n",
    "            \n",
    "            seed_feature_vectors = {}\n",
    "            seed_labels = {}\n",
    "            for seed, model_path in zip(models.keys(), model_paths):\n",
    "                if model_name == \"SimCLR\":\n",
    "                    feature_vectors, labels = extracting_feature_vectors_from_simclr(model_path, dataloader)\n",
    "                elif model_name == \"BYOL\":\n",
    "                    feature_vectors, labels = extracting_feature_vectors_from_byol(model_path, dataloader)\n",
    "                elif model_name == \"MoCo\":\n",
    "                    feature_vectors, labels = extracting_feature_vectors_from_moco(model_path, dataloader)\n",
    "\n",
    "                seed_feature_vectors[seed] = feature_vectors\n",
    "                seed_labels[seed] = labels\n",
    "\n",
    "            query_image, query_label = next(iter(dataloader))\n",
    "            query_image = query_image[0] \n",
    "            query_vector = seed_feature_vectors[\"seed 0\"][0]  \n",
    "            augmentation_name = format_model_name(model_paths[0])\n",
    "\n",
    "            nearest_neighbors_images_by_seed = {}\n",
    "            distances_by_seed = {}\n",
    "            for seed in seed_feature_vectors.keys():\n",
    "                distances = compute_distances_for_single_image(query_vector, seed_feature_vectors[seed])\n",
    "                nearest_neighbor_indices = np.argsort(distances)[:10]\n",
    "                nearest_neighbors_images = [dataloader.dataset[i][0] for i in nearest_neighbor_indices]\n",
    "                nearest_neighbors_images_by_seed[seed] = nearest_neighbors_images\n",
    "                distances_by_seed[seed] = [distances[i] for i in nearest_neighbor_indices]\n",
    "\n",
    "            plot_comparison_nearest_neighbors(\n",
    "                query_image,\n",
    "                nearest_neighbors_images_by_seed,\n",
    "                distances_by_seed,\n",
    "                title=f\"{model_name} - {dataset_name} - {augmentation_name}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2674e1-0117-4a88-9ef5-16059477a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and plot the SimCLR nearest neighbors of the query image for all models and datasets\n",
    "process_and_plot_nearest_neighbors(\"SimCLR\", simclr_models, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb17c0-0290-4c45-b886-837bd5d87816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and plot the BYOL nearest neighbors of the query image for all models and datasets\n",
    "process_and_plot_nearest_neighbors(\"BYOL\", byol_models, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41289ce0-9b79-4aac-9a89-25c51cbcfc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and plot the MoCo nearest neighbors of the query image for all models and datasets\n",
    "process_and_plot_nearest_neighbors(\"MoCo\", moco_models, datasets) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
